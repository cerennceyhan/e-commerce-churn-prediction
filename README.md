# E-Commerce Product Churn Risk Prediction ğŸš€

This project is an end-to-end data science pipeline designed to detect "Quality Churn" and "Engagement Churn" risks in e-commerce products using **LLM (Claude 4.5 Sonnet)** and **XGBoost**.

## âš ï¸ Important Privacy & Data Policy
Due to **GDPR/KVKK** regulations and strict data privacy protocols:

* **Synthetic Data Only:** The dataset included in this repository (`data/raw/sample_dataset.csv`) is **synthetic/fake**. It is generated to demonstrate the code's functionality and structure without violating privacy.
* **Real Data Privacy:** The original dataset contains real customer reviews, names, and sensitive information. Therefore, raw real data and intermediate processing files (`base_metrics.csv`, `llm_extraction.csv`) are **excluded** from this repository.
* **Proven Results:** The visualization outputs (SHAP plots, Confusion Matrices) located in the `outputs/` folder are generated using the **REAL dataset** to showcase the actual performance and validity of the model.

## ğŸ“‚ Project Structure

* `ty_scrapping.py`: Selenium-based web scraper customized for product reviews.
* `base_metrics.py`: Feature engineering module that calculates independent metrics (rating deviation, review velocity, etc.).
* `llm_extraction.py`: Advanced feature extraction using **Anthropic Claude 4.5 Sonnet API**. It analyzes unstructured text to detect specific issues like fitment problems, fabric quality, and color mismatches.
* `train_model.py`: Trains an XGBoost classifier with SMOTE oversampling and generates SHAP explanations for interpretability.
* `data/`: Contains the synthetic raw dataset (`sample_dataset.csv`) for testing.
* `outputs/`: Contains performance graphs based on real-world data analysis.

## ğŸ“Š Model Performance (on Real Data)
The model successfully differentiates between *Healthy*, *Quality Churn*, and *Engagement Churn* products using hybrid features.

*(See `outputs/` folder for detailed SHAP analysis and Confusion Matrix)*

## ğŸ› ï¸ How to Run
1.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
2.  **Environment Setup:**
    Create a `.env` file in the root directory and add your API key:
    ```text
    ANTHROPIC_API_KEY=sk-your-api-key-here
    ```
3.  **Execution Pipeline:**
    ```bash
    python base_metrics.py    # Step 1: Base stats
    python llm_extraction.py  # Step 2: LLM Analysis (Claude 4.5)
    python train_model.py     # Step 3: Training & Evaluation
    ```

---

# E-Ticaret ÃœrÃ¼n Churn (Risk) Analizi ve Tahmini ğŸ‡¹ğŸ‡·

Bu proje, e-ticaret Ã¼rÃ¼nlerindeki "Kalite KaynaklÄ± MÃ¼ÅŸteri KaybÄ±" (Quality Churn) ve "Ä°lgi KaybÄ±" (Engagement Churn) risklerini tespit etmek iÃ§in geliÅŸtirilmiÅŸ uÃ§tan uca bir veri bilimi projesidir. Projede **LLM (Claude 4.5 Sonnet)** ve **XGBoost** algoritmalarÄ± hibrit olarak kullanÄ±lmÄ±ÅŸtÄ±r.

## âš ï¸ Ã–nemli: Veri GizliliÄŸi ve KVKK PolitikasÄ±
Bu projeyi incelerken lÃ¼tfen aÅŸaÄŸÄ±daki veri gizliliÄŸi kurallarÄ±nÄ± gÃ¶z Ã¶nÃ¼nde bulundurun:

1.  **Sentetik (Fake) Veri:** Bu depoda yer alan `data/raw/sample_dataset.csv` dosyasÄ±, kodlarÄ±n Ã§alÄ±ÅŸÄ±rlÄ±ÄŸÄ±nÄ± test edebilmeniz iÃ§in oluÅŸturulmuÅŸ **tamamen sahte/sentetik** verilerdir. GerÃ§ek kiÅŸi veya kurumlarla ilgisi yoktur.
2.  **GerÃ§ek Veriler:** Projenin geliÅŸtirilmesinde kullanÄ±lan, gerÃ§ek mÃ¼ÅŸteri isimleri ve yorumlarÄ±nÄ± iÃ§eren ham veri seti ve ara iÅŸlem dosyalarÄ± (`base_metrics.csv` vb.), **KVKK (KiÅŸisel Verilerin KorunmasÄ± Kanunu)** ve gizlilik esaslarÄ± gereÄŸi bu depoda **paylaÅŸÄ±lmamÄ±ÅŸtÄ±r**.
3.  **KanÄ±tlanmÄ±ÅŸ SonuÃ§lar:** `outputs/` klasÃ¶rÃ¼nde gÃ¶receÄŸiniz grafikler (SHAP analizi, Confusion Matrix), modelin **GERÃ‡EK verilerle** eÄŸitilmesi sonucu elde edilen baÅŸarÄ±yÄ± gÃ¶stermektedir.

## ğŸ“‚ Proje Dosya YapÄ±sÄ±

* `ty_scrapping.py`: Trendyol Ã¼rÃ¼n yorumlarÄ±nÄ± Ã§ekmek iÃ§in geliÅŸtirilmiÅŸ, Selenium tabanlÄ± web kazÄ±ma botu.
* `base_metrics.py`: ÃœrÃ¼nler iÃ§in sayÄ±sal Ã¶zellikleri (puan ortalamasÄ±, yorum sÄ±klÄ±ÄŸÄ±, standart sapma vb.) hesaplayan modÃ¼l.
* `llm_extraction.py`: **Anthropic Claude 4.5 Sonnet API** kullanarak yorum metinlerini analiz eden yapay zeka modÃ¼lÃ¼. Metinlerden "kalÄ±p hatasÄ±", "kumaÅŸ kalitesi", "renk uyuÅŸmazlÄ±ÄŸÄ±" gibi spesifik sorunlarÄ± tespit eder.
* `train_model.py`: Elde edilen tÃ¼m Ã¶zellikleri birleÅŸtirerek XGBoost modeli ile risk tahmini yapar. SMOTE ile veri dengesizliÄŸini giderir ve SHAP kÃ¼tÃ¼phanesi ile modelin kararlarÄ±nÄ± aÃ§Ä±klar.
* `outputs/`: Modelin gerÃ§ek veriler Ã¼zerindeki performans grafiklerini iÃ§erir.

## ğŸ“Š Model PerformansÄ±
Model, sayÄ±sal veriler ve LLM'den gelen iÃ§gÃ¶rÃ¼leri birleÅŸtirerek Ã¼rÃ¼nleri 3 sÄ±nÄ±fa ayÄ±rmaktadÄ±r:
1.  **Healthy (SaÄŸlÄ±klÄ±):** Sorunsuz Ã¼rÃ¼nler.
2.  **Quality Churn Risk:** Kalite, kalÄ±p veya kumaÅŸ sorunu olan iade riski yÃ¼ksek Ã¼rÃ¼nler.
3.  **Engagement Churn Risk:** Yorum sayÄ±sÄ± az veya fiyat/performans dengesi bozuk Ã¼rÃ¼nler.

*(DetaylÄ± performans grafikleri iÃ§in `outputs` klasÃ¶rÃ¼ne bakabilirsiniz.)*

## ğŸ› ï¸ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

1.  **Gerekli KÃ¼tÃ¼phaneler:**
    ```bash
    pip install -r requirements.txt
    ```

2.  **API AyarlarÄ±:**
    Ana dizinde `.env` dosyasÄ± oluÅŸturun ve Anthropic API anahtarÄ±nÄ±zÄ± ekleyin:
    ```text
    ANTHROPIC_API_KEY=sk-ant-api03-...
    ```

3.  **Ã‡alÄ±ÅŸtÄ±rma SÄ±rasÄ±:**
    ```bash
    python base_metrics.py    # AdÄ±m 1: Temel metrikleri Ã§Ä±kar
    python llm_extraction.py  # AdÄ±m 2: Yapay zeka (Claude 4.5) ile yorumlarÄ± analiz et
    python train_model.py     # AdÄ±m 3: Modeli eÄŸit ve sonuÃ§larÄ± Ã¼ret
    ```

## ğŸ“ HazÄ±rlayan / Author
**Ceren Ceyhan**
